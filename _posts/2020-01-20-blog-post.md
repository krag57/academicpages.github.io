---
title: 'Blog Post number 1'
date: 2020-01-20
permalink: /posts/2020/01/blog-post/
tags:
  - cool posts
  - category1
  - category2
---

Every year there are thousands of people become victims for fraud credit card transactions. What is a credit card fraud. It is when someone uses your credit card or credit account without your permission. While there are several ways to punish those criminals, it is equally importance to identify those transactions. The quicker the theft is identified; the lesser the effects could be.

A dataset of credit card transactions of 5000 customers is given, denoting whether it is fraud or not.  This dataset also contains various other information about these transactions, such as account number, merchant name, and so on. In addition to these transactions, the information about the cards also given. 

The aim of this project is to build a machine learning algorithm to classify whether these transactions are fraud or not. As the predictive model, a tree based random forest model has been built. The model could be able to predict the fraudulent transactions with the accuracy of 82%. 

This report discusses the steps involved in the modelling. Under section one, a descriptive study of variables is carried out. The next section is helped to identify the properties of variable transaction Amount. Then the section 3 is dedicated to data wrangling and removing duplicate transactions. The 4th section discusses how the model is built. Finally, a dedicated section is included for conclusion and discussions.


## Introduction

Every year there are thousands of people become victims for fraud credit card transactions. What is a credit card fraud. It is when someone uses your credit card or credit account without your permission. While there are several ways to punish those criminals, it is equally importance to identify those transactions. The quicker the theft is identified; the lesser the effects could be.

A dataset of credit card transactions of 5000 customers is given, denoting whether it is fraud or not.  This dataset also contains various other information about these transactions, such as account number, merchant name, and so on. In addition to these transactions, the information about the cards also given. 

The aim of this project is to build a machine learning algorithm to classify whether these transactions are fraud or not. As the predictive model, a tree based random forest model has been built. The model could be able to predict the fraudulent transactions with the accuracy of 82%. 

This report discusses the steps involved in the modelling. Under section one, a descriptive study of variables is carried out. The next section is helped to identify the properties of variable transaction Amount. Then the section 3 is dedicated to data wrangling and removing duplicate transactions. The 4th section discusses how the model is built. Finally, a dedicated section is included for conclusion and discussions.

#### Importing necessary packages


```python
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import matplotlib.gridspec as gridspec
import warnings
import datetime
from sklearn.preprocessing import LabelEncoder, OneHotEncoder, OrdinalEncoder
from sklearn.model_selection import train_test_split,cross_val_score,RandomizedSearchCV
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, confusion_matrix,roc_auc_score, roc_curve
```

## 1. Load

### 1.1 Download  and load into your favorite analytical tool the transactions data. 


```python
#reading Json file
df=pd.read_json("transactions.zip",lines=True)
df.head()
df.replace("",np.nan,inplace=True)
```

### 1.2 Describe the structure of the data


```python
#Structure of the dataset
df.shape
print("Total number of rows:",df.shape[0])
print("Total number of columns:",df.shape[1])
```

    Total number of rows: 786363
    Total number of columns: 29



```python
# Variable names
df.columns 
```




    Index(['accountNumber', 'customerId', 'creditLimit', 'availableMoney',
           'transactionDateTime', 'transactionAmount', 'merchantName',
           'acqCountry', 'merchantCountryCode', 'posEntryMode', 'posConditionCode',
           'merchantCategoryCode', 'currentExpDate', 'accountOpenDate',
           'dateOfLastAddressChange', 'cardCVV', 'enteredCVV', 'cardLast4Digits',
           'transactionType', 'echoBuffer', 'currentBalance', 'merchantCity',
           'merchantState', 'merchantZip', 'cardPresent', 'posOnPremises',
           'recurringAuthInd', 'expirationDateKeyInMatch', 'isFraud'],
          dtype='object')




```python
### setting the time variables as this will help in future analysis
df['currentExpDate']=pd.to_datetime(df['currentExpDate'])
df['transactionDateTime']=pd.to_datetime(df['transactionDateTime'])
df['accountOpenDate']=pd.to_datetime(df['accountOpenDate'])
df['dateOfLastAddressChange']=pd.to_datetime(df['dateOfLastAddressChange'])
```


```python
#numerical data
df.select_dtypes(exclude=['int64','float64']).shape[1] 
```




    14



### 1.3 Summary statistics of data


```python
#Numerical Data summary
df.describe()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>accountNumber</th>
      <th>customerId</th>
      <th>creditLimit</th>
      <th>availableMoney</th>
      <th>transactionAmount</th>
      <th>cardCVV</th>
      <th>enteredCVV</th>
      <th>cardLast4Digits</th>
      <th>echoBuffer</th>
      <th>currentBalance</th>
      <th>merchantCity</th>
      <th>merchantState</th>
      <th>merchantZip</th>
      <th>posOnPremises</th>
      <th>recurringAuthInd</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>count</td>
      <td>7.863630e+05</td>
      <td>7.863630e+05</td>
      <td>786363.000000</td>
      <td>786363.000000</td>
      <td>786363.000000</td>
      <td>786363.000000</td>
      <td>786363.000000</td>
      <td>786363.000000</td>
      <td>0.0</td>
      <td>786363.000000</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
    </tr>
    <tr>
      <td>mean</td>
      <td>5.372326e+08</td>
      <td>5.372326e+08</td>
      <td>10759.464459</td>
      <td>6250.725369</td>
      <td>136.985791</td>
      <td>544.467338</td>
      <td>544.183857</td>
      <td>4757.417799</td>
      <td>NaN</td>
      <td>4508.739089</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
    </tr>
    <tr>
      <td>std</td>
      <td>2.554211e+08</td>
      <td>2.554211e+08</td>
      <td>11636.174890</td>
      <td>8880.783989</td>
      <td>147.725569</td>
      <td>261.524220</td>
      <td>261.551254</td>
      <td>2996.583810</td>
      <td>NaN</td>
      <td>6457.442068</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
    </tr>
    <tr>
      <td>min</td>
      <td>1.000881e+08</td>
      <td>1.000881e+08</td>
      <td>250.000000</td>
      <td>-1005.630000</td>
      <td>0.000000</td>
      <td>100.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>NaN</td>
      <td>0.000000</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
    </tr>
    <tr>
      <td>25%</td>
      <td>3.301333e+08</td>
      <td>3.301333e+08</td>
      <td>5000.000000</td>
      <td>1077.420000</td>
      <td>33.650000</td>
      <td>310.000000</td>
      <td>310.000000</td>
      <td>2178.000000</td>
      <td>NaN</td>
      <td>689.910000</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
    </tr>
    <tr>
      <td>50%</td>
      <td>5.074561e+08</td>
      <td>5.074561e+08</td>
      <td>7500.000000</td>
      <td>3184.860000</td>
      <td>87.900000</td>
      <td>535.000000</td>
      <td>535.000000</td>
      <td>4733.000000</td>
      <td>NaN</td>
      <td>2451.760000</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
    </tr>
    <tr>
      <td>75%</td>
      <td>7.676200e+08</td>
      <td>7.676200e+08</td>
      <td>15000.000000</td>
      <td>7500.000000</td>
      <td>191.480000</td>
      <td>785.000000</td>
      <td>785.000000</td>
      <td>7338.000000</td>
      <td>NaN</td>
      <td>5291.095000</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
    </tr>
    <tr>
      <td>max</td>
      <td>9.993896e+08</td>
      <td>9.993896e+08</td>
      <td>50000.000000</td>
      <td>50000.000000</td>
      <td>2011.540000</td>
      <td>998.000000</td>
      <td>998.000000</td>
      <td>9998.000000</td>
      <td>NaN</td>
      <td>47498.810000</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
    </tr>
  </tbody>
</table>
</div>




```python
#Non numerical data summary
df.describe(exclude=['int64','float64'])
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>transactionDateTime</th>
      <th>merchantName</th>
      <th>acqCountry</th>
      <th>merchantCountryCode</th>
      <th>posEntryMode</th>
      <th>posConditionCode</th>
      <th>merchantCategoryCode</th>
      <th>currentExpDate</th>
      <th>accountOpenDate</th>
      <th>dateOfLastAddressChange</th>
      <th>transactionType</th>
      <th>cardPresent</th>
      <th>expirationDateKeyInMatch</th>
      <th>isFraud</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>count</td>
      <td>786363</td>
      <td>786363</td>
      <td>781801</td>
      <td>785639</td>
      <td>782309</td>
      <td>785954</td>
      <td>786363</td>
      <td>786363</td>
      <td>786363</td>
      <td>786363</td>
      <td>785665</td>
      <td>786363</td>
      <td>786363</td>
      <td>786363</td>
    </tr>
    <tr>
      <td>unique</td>
      <td>776637</td>
      <td>2490</td>
      <td>4</td>
      <td>4</td>
      <td>5</td>
      <td>3</td>
      <td>19</td>
      <td>165</td>
      <td>1820</td>
      <td>2184</td>
      <td>3</td>
      <td>2</td>
      <td>2</td>
      <td>2</td>
    </tr>
    <tr>
      <td>top</td>
      <td>2016-05-28 14:24:41</td>
      <td>Uber</td>
      <td>US</td>
      <td>US</td>
      <td>05</td>
      <td>01</td>
      <td>online_retail</td>
      <td>2029-03-01 00:00:00</td>
      <td>2014-06-21 00:00:00</td>
      <td>2016-03-15 00:00:00</td>
      <td>PURCHASE</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <td>freq</td>
      <td>4</td>
      <td>25613</td>
      <td>774709</td>
      <td>778511</td>
      <td>315035</td>
      <td>628787</td>
      <td>202156</td>
      <td>5103</td>
      <td>33623</td>
      <td>3819</td>
      <td>745193</td>
      <td>433495</td>
      <td>785320</td>
      <td>773946</td>
    </tr>
    <tr>
      <td>first</td>
      <td>2016-01-01 00:01:02</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>2019-12-01 00:00:00</td>
      <td>1989-08-22 00:00:00</td>
      <td>1989-08-22 00:00:00</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
    </tr>
    <tr>
      <td>last</td>
      <td>2016-12-30 23:59:45</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>2033-08-01 00:00:00</td>
      <td>2015-12-31 00:00:00</td>
      <td>2016-12-30 00:00:00</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
    </tr>
  </tbody>
</table>
</div>




```python
# Total number of NULL values in each columns
dfNull=df.isnull().sum()
dfNull[dfNull>0].to_frame().rename(columns={0:"Total Null"}).T
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>acqCountry</th>
      <th>merchantCountryCode</th>
      <th>posEntryMode</th>
      <th>posConditionCode</th>
      <th>transactionType</th>
      <th>echoBuffer</th>
      <th>merchantCity</th>
      <th>merchantState</th>
      <th>merchantZip</th>
      <th>posOnPremises</th>
      <th>recurringAuthInd</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Total Null</td>
      <td>4562</td>
      <td>724</td>
      <td>4054</td>
      <td>409</td>
      <td>698</td>
      <td>786363</td>
      <td>786363</td>
      <td>786363</td>
      <td>786363</td>
      <td>786363</td>
      <td>786363</td>
    </tr>
  </tbody>
</table>
</div>



There are $786363$ records and $29$ variables of 5000 unique account numbers. The set of variables consist of numerical and categorical variables. $15$ of $29$ are numerical and rest are non-numerical data. Date variables are also included with in this numerical set of variables. The above table shows number of NULL values in the set. As in the introduction the columns which had no information are `merchantCity`, `merchantState`, `merchantZip`, `posOnPremises`, `recurringAuthInd`, `echoBuffer`. Apart from these null values, there are variables with small amount of NULL values.

Another interesting fact is there are 2490 merchants included in the dataset. The time related variables are transaction date, exp date of credit card, address change date, and account open date. The type of transactions included in the study are purchase, reversal, account verification. 


## 2. Plot

### 2.1 Plot a histogram `transactionAmount` column.


```python
#Figures to represent the transaction amount
fig,(ax1,ax2)=plt.subplots(1,2,figsize=(20,5))
fig.suptitle("Histograms",  fontsize=24)
_=ax1.hist(df['transactionAmount'],bins=15)
_=ax1.set_title("15 bins", fontsize=18)
_=ax1.set_ylabel("Frequency", fontsize=15)
_=ax1.set_xlabel("Amount($)", fontsize=15)


_=ax2.hist(df['transactionAmount'],bins=30,color='orange')
_=ax2.set_title("30 bins", fontsize=18)
_=ax2.set_ylabel("Frequency", fontsize=15)
_=ax2.set_xlabel("Amount($)", fontsize=15)

```


![png](/files/images/output_21_0.png)


### 2.2 Structure from histogram

The histogram given by above plot discusses the distribution of transactions. There are two cases presented, what is the difference among them? Well, one with few more bins than the other. The one with more bins helps to visualize that there are huge number of transactions with small amount probably, less than $\$50$. The first plot shows that there is significantly large number of transactions less than $\$125$, when this is compared with the rest of the values. 

It is also clear that the distribution is rightly skewed. This information might be useful, if we ever try to use a regression-based model.

## 3. Data Wrangling - Duplicate Transactions

### 3.1 Handling missing values


```python
# Total number of NULL values by features
dfNull=df.isnull().sum()
dfNull[dfNull>0].to_frame().rename(columns={0:"Total Null"}).T
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>acqCountry</th>
      <th>merchantCountryCode</th>
      <th>posEntryMode</th>
      <th>posConditionCode</th>
      <th>transactionType</th>
      <th>echoBuffer</th>
      <th>merchantCity</th>
      <th>merchantState</th>
      <th>merchantZip</th>
      <th>posOnPremises</th>
      <th>recurringAuthInd</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Total Null</td>
      <td>4562</td>
      <td>724</td>
      <td>4054</td>
      <td>409</td>
      <td>698</td>
      <td>786363</td>
      <td>786363</td>
      <td>786363</td>
      <td>786363</td>
      <td>786363</td>
      <td>786363</td>
    </tr>
  </tbody>
</table>
</div>




```python
#Treating the NULL values
df.dropna(axis=1,how='all',inplace=True)#Entirely removing certain variables
df.dropna(axis=0,how='any',inplace=True)#Removing the some observations
```

There are several ways to deal with the missing or NULL values in the dataset. But if the variables contain no information it is useless to keep them. So, we removed all such variables. 

But you can notice there are several categorical variables with NULL values, how to deal with these? As general practices, someone could ignore those, if they are dealing with large data sets and a smaller number of records missing or else ignore variable, if it is not significant. So, the observations are ignored.

### 3.2 Handling the reversed transaction

Through this section we tried to develop a program to identify the reversed transactions. Reversed transactions are when customer receive back the funds. This could happen in two ways; authorization reversal, and refunding payment. 


```python
#Identifying the Reversed transactions
dfRev=df.loc[df['transactionType']=="REVERSAL",['accountNumber','transactionAmount','transactionType']]
dfPur=df.loc[df['transactionType']=="PURCHASE",['accountNumber','transactionAmount','transactionType']]
dfRev.head() #Displaying the some of the observations
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>accountNumber</th>
      <th>transactionAmount</th>
      <th>transactionType</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>39</td>
      <td>574788567</td>
      <td>215.13</td>
      <td>REVERSAL</td>
    </tr>
    <tr>
      <td>73</td>
      <td>574788567</td>
      <td>3.87</td>
      <td>REVERSAL</td>
    </tr>
    <tr>
      <td>101</td>
      <td>924729945</td>
      <td>168.57</td>
      <td>REVERSAL</td>
    </tr>
    <tr>
      <td>133</td>
      <td>984504651</td>
      <td>450.74</td>
      <td>REVERSAL</td>
    </tr>
    <tr>
      <td>156</td>
      <td>984504651</td>
      <td>81.73</td>
      <td>REVERSAL</td>
    </tr>
  </tbody>
</table>
</div>




```python
#No of reversed transactions
print('The total number of reversed transaction are:', dfRev.shape[0])
print('The total dollar amount reversed:',dfRev['transactionAmount'].sum())
```

    The total number of reversed transaction are: 20062
    The total dollar amount reversed: 2787895.03



```python
#Summary statistics of those transactions
dfRev.describe()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>accountNumber</th>
      <th>transactionAmount</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>count</td>
      <td>2.006200e+04</td>
      <td>20062.000000</td>
    </tr>
    <tr>
      <td>mean</td>
      <td>5.358785e+08</td>
      <td>138.963963</td>
    </tr>
    <tr>
      <td>std</td>
      <td>2.550042e+08</td>
      <td>147.737618</td>
    </tr>
    <tr>
      <td>min</td>
      <td>1.000881e+08</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <td>25%</td>
      <td>3.297486e+08</td>
      <td>33.372500</td>
    </tr>
    <tr>
      <td>50%</td>
      <td>4.992138e+08</td>
      <td>92.015000</td>
    </tr>
    <tr>
      <td>75%</td>
      <td>7.676200e+08</td>
      <td>192.970000</td>
    </tr>
    <tr>
      <td>max</td>
      <td>9.992836e+08</td>
      <td>1435.640000</td>
    </tr>
  </tbody>
</table>
</div>




```python
#ploting the Reversed and Purchased Transactions
plt.figure(figsize=(20,10))
gspec = gridspec.GridSpec(2, 2)
rb = plt.subplot(gspec[1, 1])
rt = plt.subplot(gspec[0, 1])
le = plt.subplot(gspec[0:, 0])

dfPurSum=dfPur[['accountNumber','transactionAmount']].groupby("accountNumber").sum().reset_index()
dfRevSum=dfRev[['accountNumber','transactionAmount']].groupby("accountNumber").sum().reset_index()


le.boxplot(dfRev["transactionAmount"],positions=[1])
le.boxplot(dfPur["transactionAmount"],positions=[2])
ticks = ['Reversed', 'Purchased']
_=le.set_xticks([1,2], ticks)
_=le.set_title("Distribution of Transaction types", fontsize=18)
_=le.set_ylabel("Amount($)", fontsize=15)
_=le.set_xlabel("Transaction types", fontsize=15)


rb.plot(np.arange(0,dfRevSum.shape[0]),dfRevSum["transactionAmount"],'-r')
_=rb.set_title("Distribution of Reversed", fontsize=15)
_=rb.set_ylabel("Amount($)", fontsize=12)
_=rb.set_xlabel("Indices", fontsize=12)

rt.plot(np.arange(0,dfPurSum.shape[0]),dfPurSum["transactionAmount"])
_=rt.set_title("Distribution of Purchased", fontsize=15)
_=rt.set_ylabel("Amount($)", fontsize=12)
_=rt.set_xlabel("Indices", fontsize=12)
```


![png](/files/images/output_34_0.png)


The total number of reversed transactions are $20062$ and the total dollar amount reversed is $\$2787895.03$. 


The above box plots show the distribution of reversed transactions and purchased transactions. They are almost same distribution-wise with heavy tails. But, the next plots show the total number of reversed and purchased transactions from each customer. It is noticeable that there are around $\$120000$ reversed from a customer and that person is also responsible for huge amount of total purchase. Is this a outlier or not?  We need more information to identify.  

### 3.3 Handling the multi-swip transaction

These are another type of duplicate transections happen when the vendor accidentally charges a customer's card multiple times within a short time span. 


```python
dfNo_Rev=df.loc[df['transactionType']!="REVERSAL",:]
def multiswip(data=dfNo_Rev,t=2):
    
    '''
    Function to identify the multi-swip transactions
    1. First we are sorting based on account number and transaction time
    2. Then we are finding the duplicated transaction amount based on account number.
    3. Now we are Joining the previous steps with where swip occur with in t minutes
    '''
    dfNo_Rev.sort_values(['accountNumber', 'transactionDateTime'], inplace=True)
    f1 = dfNo_Rev.groupby('accountNumber', sort=False)['transactionAmount'].apply(lambda x: x.duplicated())
    f2 = dfNo_Rev.groupby('accountNumber', sort=False)['transactionDateTime'].diff() <= pd.Timedelta(t, unit='minutes')
    dfNo_Rev.loc[:,'Duplicated'] = np.where(f1 & f2, 'Yes', 'No')
    
    warnings.filterwarnings("ignore")
    return dfNo_Rev['Duplicated'].value_counts()


def plot_time(intial=1,end=5):
    mu=[]
    for i in range(intial,end):
       mu.append(multiswip(t=i)[1])
    
    plt.figure(figsize=(16,8))
    plt.plot(range(intial,end),mu,'-o')
    _=plt.title("Multi-swiped transactions", fontsize=18)
    _=plt.ylabel("# Multi-swiped transactions", fontsize=15)
    _=plt.xlabel("Time (minutes)", fontsize=15)


    
plot_time(intial=1,end=10)

```

    /Users/student/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:10: SettingWithCopyWarning: 
    A value is trying to be set on a copy of a slice from a DataFrame
    
    See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
      # Remove the CWD from sys.path while we load stuff.
    /Users/student/opt/anaconda3/lib/python3.7/site-packages/pandas/core/indexing.py:376: SettingWithCopyWarning: 
    A value is trying to be set on a copy of a slice from a DataFrame.
    Try using .loc[row_indexer,col_indexer] = value instead
    
    See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
      self.obj[key] = _infer_fill_value(value)
    /Users/student/opt/anaconda3/lib/python3.7/site-packages/pandas/core/indexing.py:494: SettingWithCopyWarning: 
    A value is trying to be set on a copy of a slice from a DataFrame.
    Try using .loc[row_indexer,col_indexer] = value instead
    
    See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
      self.obj[item] = s



![png](/files/images/output_38_1.png)


To identify the multi-swap transactions a program is created through following steps. First the dataset is sorted based on account number and transaction time then using this the duplicated transaction amount is identified among account numbers (Grouped based on account number). Then same amount transaction within $t$ minutes are marked as multi-swap and ignored all the other transactions by keeping only one. 

Since threshold time is not given, the number of such transactions is considered between t=1 to t=10. The plot above shows that. It would be ideal consider the threshold time as any value less than 2 

## 4. Model

Under this section the steps involved in building the predictive model to determine whether a given transaction is fraudulent or not is discussed. We have considered all the information.

### 4.1 feature engineering

Here are the steps involved in selecting features and at the same time ignoring some of those according to the importance to the model that is to be built. That is to classify fraudulent transactions. There are altogether 23 features or variables. Among them the aim is to use most of them. So, following are the few steps carried out to make our analysis far more interesting.

1. First the variable `accountNumber` removed as it is a duplication of `customerId`. 


2. Next access the importance of `cardCVV` and `enteredCVV`.  For that a new feature is created that will be binary encoded variable based on the equality of two features. 


3. To handle the datetime variables the features among them is extracted such as, the year, month, day, hour and considered them as separate features (categorical variables). Normally the issue of doing so  would increase the predictor set. But for our case this will not be a problem as the dataset is too large.


4. The plan is to implement a machine learning algorithm. Most of them are prefer to work with numbers so the categories of categorical variables are converted from text to numbers.


```python
# dropping the accountNumber column
dfNo_Rev.drop('accountNumber',axis=1,inplace=True)

#dropping the duplicated column
dfNo_Rev.drop('Duplicated',axis=1,inplace=True)

#creating binary variable based on CVV information
dfNo_Rev["card_enterd"]=dfNo_Rev['cardCVV'].eq(dfNo_Rev['enteredCVV'])

#dropping the cardCVV, enteredCVV
dfNo_Rev.drop(['cardCVV','enteredCVV'],axis=1,inplace=True)

# extracting features from date-time variables.
dt_names=['transactionDateTime','currentExpDate', 'accountOpenDate','dateOfLastAddressChange']

dfNo_Rev['TD_hour']=dfNo_Rev['transactionDateTime'].dt.hour
dfNo_Rev['TD_day']=dfNo_Rev['transactionDateTime'].dt.day
dfNo_Rev['TD_month']=dfNo_Rev['transactionDateTime'].dt.month
dfNo_Rev['TD_year']=dfNo_Rev['transactionDateTime'].dt.year

dfNo_Rev['Ex_month']=dfNo_Rev['currentExpDate'].dt.month
dfNo_Rev['Ex_year']=dfNo_Rev['currentExpDate'].dt.year

dfNo_Rev['AO_day']=dfNo_Rev['accountOpenDate'].dt.day
dfNo_Rev['AO_month']=dfNo_Rev['accountOpenDate'].dt.month
dfNo_Rev['AO_year']=dfNo_Rev['accountOpenDate'].dt.year

dfNo_Rev['LAC_day']=dfNo_Rev['dateOfLastAddressChange'].dt.day
dfNo_Rev['LAC_month']=dfNo_Rev['dateOfLastAddressChange'].dt.month
dfNo_Rev['LAC_year']=dfNo_Rev['dateOfLastAddressChange'].dt.year

dfNo_Rev.drop(dt_names,axis=1,inplace=True)


# coverting text category levels to numbers.
dfNo_Rev.isFraud=dfNo_Rev.isFraud.astype(int)
dfNo_Rev.cardPresent=dfNo_Rev.cardPresent.astype(int)
dfNo_Rev.expirationDateKeyInMatch=dfNo_Rev.expirationDateKeyInMatch.astype(int)
#dfNo_Rev.card_enterd=dfNo_Rev.card_enterd.astype(int)

ordinal_encoder=OrdinalEncoder()
dfNo_Rev.transactionType=ordinal_encoder.fit_transform(dfNo_Rev[['transactionType']]) #['ADDRESS_VERIFICATION', 'PURCHASE']
dfNo_Rev.merchantName=ordinal_encoder.fit_transform(dfNo_Rev[['merchantName']]) #['1st BBQ', '1st Deli', '1st Pub'...]
dfNo_Rev.acqCountry=ordinal_encoder.fit_transform(dfNo_Rev[['acqCountry']]) #['CAN', 'MEX', 'PR', 'US']
dfNo_Rev.merchantCountryCode=ordinal_encoder.fit_transform(dfNo_Rev[['merchantCountryCode']]) #['CAN', 'MEX', 'PR', 'US']
dfNo_Rev.posEntryMode=ordinal_encoder.fit_transform(dfNo_Rev[['posEntryMode']]) #['02', '05', '09', '80', '90']
dfNo_Rev.posConditionCode=ordinal_encoder.fit_transform(dfNo_Rev[['posConditionCode']]) #['01', '08', '99']
dfNo_Rev.merchantCategoryCode=ordinal_encoder.fit_transform(dfNo_Rev[['merchantCategoryCode']]) #['airline', 'auto', 'cable/phone',...]


```

#### 4.1.1 correlation analysis

These are the numerical variables `customerId`,`creditLimit`, `availableMoney`,`transactionAmount`, `cardLast4Digits`,`currentBalance`. The correlations among them are accessed.





```python
#Correlation Analysis
nu_names=['customerId','creditLimit', 'availableMoney','transactionAmount','cardLast4Digits','currentBalance'] # numerical variables
corr=dfNo_Rev[nu_names].corr()
corr.style.background_gradient(cmap='Blues').set_properties(**{'font-size': '12pt'}).set_precision(2)
```




![png](/files/images/corre.png)


1. There is a significantly high correlation between `availableMoney` and `creditLimit`. This fact is kind of obvious, isn't it?


2. Then there is a strong correlation between `currentBalance` and `creditLimit`. 


3. There are no significant correlation between other variables. This is a good fact as this might come handy in modelling.


```python
#Checking the types of variables for the lasttime before modelling
dfNo_Rev.info()
```

    <class 'pandas.core.frame.DataFrame'>
    Int64Index: 756606 entries, 541900 to 108115
    Data columns (total 29 columns):
    customerId                  756606 non-null int64
    creditLimit                 756606 non-null int64
    availableMoney              756606 non-null float64
    transactionAmount           756606 non-null float64
    merchantName                756606 non-null float64
    acqCountry                  756606 non-null float64
    merchantCountryCode         756606 non-null float64
    posEntryMode                756606 non-null float64
    posConditionCode            756606 non-null float64
    merchantCategoryCode        756606 non-null float64
    cardLast4Digits             756606 non-null int64
    transactionType             756606 non-null float64
    currentBalance              756606 non-null float64
    cardPresent                 756606 non-null int64
    expirationDateKeyInMatch    756606 non-null int64
    isFraud                     756606 non-null int64
    card_enterd                 756606 non-null bool
    TD_hour                     756606 non-null int64
    TD_day                      756606 non-null int64
    TD_month                    756606 non-null int64
    TD_year                     756606 non-null int64
    Ex_month                    756606 non-null int64
    Ex_year                     756606 non-null int64
    AO_day                      756606 non-null int64
    AO_month                    756606 non-null int64
    AO_year                     756606 non-null int64
    LAC_day                     756606 non-null int64
    LAC_month                   756606 non-null int64
    LAC_year                    756606 non-null int64
    dtypes: bool(1), float64(10), int64(18)
    memory usage: 188.1 MB


### 4.2 Model building

Here let’s see the model building procedure. According to the problem we have to build a predictive model. But there are several models available in the machine learning platform. So, we have to choose a correct model based on several criterion. So, first things first, before appropriately choosing the model, we have few questions to address.

1. What kind of machine learning algorithms should we use, is it classification or regression?
**We are going to use classification as we are going to build a predictive model based on the labels**


2. What kind of classification algorithm?
**There are several algorithms currently available. Since this is a large $n$ problem, it is not possible to use all the algorithms. Therefore, we decided to use random forest**


3. Why random forest? 
**It is one of most powerful algorithms available. It provides higher accuracy. If there are more trees, the Random forest classifier won’t allow overfitting trees in the model. It has the power to handle a large data set with higher dimensionality efficiently**


4. Is feature selection posible?
**Yes, it is possible**


5. Is scaling is required?
**No. Random Forests are based on tree partitioning algorithms. As the trees only see ranks in the features.**


```python
#We’ll use train-test-split to split the data into training data and testing data.
X=dfNo_Rev.drop('isFraud',axis=1)
y=dfNo_Rev.isFraud
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=70) # 30% test size

RFC = RandomForestClassifier()
RFC.fit(X_train,y_train) #fitting RF model
```




    RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',
                           max_depth=None, max_features='auto', max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_impurity_split=None,
                           min_samples_leaf=1, min_samples_split=2,
                           min_weight_fraction_leaf=0.0, n_estimators=10,
                           n_jobs=None, oob_score=False, random_state=None,
                           verbose=0, warm_start=False)



The dataset is split into training and testing. 30% of the samples is taken into test set and rest to the training.


```python
# predictions
RFC_predict = RFC.predict(X_test)
RFC_predict_prob=RFC.predict_proba(X_test)[:,1]
```


```python
# The Confusion metric
CM=confusion_matrix(y_test, RFC_predict)

def plot_ConMatrix_ROCcurve(cm,RFC_predict_prob):
    '''
    This function plot Confusion matrix and Roc curve.
    
    arguments:
    1. cm=confusion Matrix
    2. RFC_predict_prob= probabilities of predictions
    
    '''
    fig,(ax1,ax2)=plt.subplots(1,2,figsize=(20,8))
    #plt.figure(figsize=(10,5))
    index = columns =['Non-Fraud','Fraud'] 
    cm_df = pd.DataFrame(CM,columns,index) 
    ax=sns.heatmap(cm_df,cmap='Blues',ax=ax1)
    ax.set_ylabel('True label',fontsize=15)
    ax.set_xlabel('Predicted label',fontsize=15)
    _=ax1.text(0.25, 0.25,str(cm[1,0]),horizontalalignment='center',verticalalignment='center',transform = ax.transAxes,fontsize=15)
    _=ax1.text(0.25, 0.75,str(cm[0,0]),horizontalalignment='center',verticalalignment='center',transform = ax.transAxes,fontsize=15)
    _=ax1.text(0.75, 0.25,str(cm[1,1]),horizontalalignment='center',verticalalignment='center',transform = ax.transAxes,fontsize=15)
    _=ax1.text(0.75, 0.75,str(cm[0,1]),horizontalalignment='center',verticalalignment='center',transform = ax.transAxes,fontsize=15)
    #ax.set_ylim([.5, 1.5])

    # ROC_AUC curves
    b_fpr, b_tpr, _ = roc_curve(y_test, [1 for _ in range(len(y_test))])
    m_fpr, m_tpr, _ = roc_curve(y_test, RFC_predict_prob)
    plt.plot(b_fpr, b_tpr, 'b', label = 'baseline')
    plt.plot(m_fpr, m_tpr, 'r', label = 'model')
    #ax2.rcParams['font.size'] = 16
    ax2.legend();
    ax2.set_xlabel('False Positive Rate'); 
    ax2.set_ylabel('True Positive Rate');
    ax2.set_title('ROC Curves')

    
plot_ConMatrix_ROCcurve(cm=CM,RFC_predict_prob=RFC_predict_prob)
print("The ROC AUC score:", roc_auc_score(y_test, RFC_predict_prob))
```

    The ROC AUC score: 0.6814273886586781



![png](/files/images/output_55_1.png)


The confusion matrix is a metric which often used to measure the performance of a classification model. It is the summary of  predicted results compared with the actual results. The number of corrected and incorrect predictions are presented in a tabular form as the summary. It gives us insight not only into the errors being made by a classifier but more importantly the types of errors that are being made. Through this metric the accuracy of the model and the precision is evaluated.

|                     | Predicted<br>Non-Fraud | Predicted<br>Fraud |
|---------------------|:------------------:|------------------------|
| **Actual<br>Non-Fraud**     | TP           | FN                    |
| **Actual<br>Fraud** | FP               | TN                  |


The Accuracy of the model is given by 

$$Accuracy=\frac{TP+TN}{TP+TN+FP+FN}$$


$$Accuracy=\frac{223582+206}{223582+206+18+3176}=0.99$$

Normally accuracy of this much considered to be good, but in our case it won't mean much. Because the accuracy is not a reliable metric for the performance of a classifier, if the data set is unbalanced 


Due to above mentioned issue we turned to Receiver operating characteristic curve (ROC) and area under the curve (AUC). These curves also used to measure the performance of classification model. While ROC shows the performance of the model under various threshold, the AUC measures the area underneath the ROC curve. The metric tells how well a particular classification model distinguishably identify the classes.  So, Higher the AUC, better the model is at distinguishing between classes.

The ROC curve is plotted with True Positive Rate (TPR) against the (False Positive Rate) FPR. The closer the curve follows the top-left hand border of the ROC space, the more accurate the test.

$$ TPR=\frac{TP}{TP+FN}$$
$$FNR=\frac{FN}{FN+TP}$$

The final testing ROC AUC for the random forest is 0.68 with an unlimited max depth. The score isn’t bad but let’s see whether the model can be improved or not by tuning the hyper parameters


```python
## Perform CV for to increase the accuracy
RFC_cv_score = cross_val_score(RFC, X, y, cv=10, scoring='roc_auc')
print("All AUC Scores: ",RFC_cv_score)
print('\n')
print("Mean AUC Score - Random Forest: ", RFC_cv_score.mean())
```

    All AUC Scores:  [0.55969688 0.57607872 0.58343667 0.58763641 0.51226207 0.57680558
     0.5785596  0.54921422 0.43601886 0.5377667 ]
    
    
    Mean AUC Score - Random Forest:  0.5497475722461519


The cross-validation technique is use to evaluate the model, based on how well they perform outside the training set to a new dataset which is test set. In other terms this might give us an idea of the model's ability to predict unseen data. 


In our case a 10-fold cross-validation is used with the focus of improving the performance, in which the model is trained using 9 folds of the data and tested on the remaining one. This procedure is carried out until the model is tested on all of the folds one by one. 

The result shows the model has a high precision as the results are close to each other. Since all of them are pretty equal



```python
# Extract feature importances
import_features=pd.DataFrame({'Feature': list(X_train.columns),
                   'Importance': RFC.feature_importances_}).\
                    sort_values('Importance', ascending = False)

# Display
import_features
plt.figure(figsize=(20,8))
bplot=sns.barplot(x='Feature',y='Importance',data=import_features)
_=plt.setp(bplot.get_xticklabels(), rotation=45,horizontalalignment='right')
bplot.set_ylabel('Importance',fontsize=15)
bplot.set_xlabel('Features',fontsize=15)
bplot.set_title("Feature Importance plot",fontsize=20)
    
```




    Text(0.5, 1.0, 'Feature Importance plot')




![png](/files/images/output_59_1.png)


Another important feature of the random forest is, it allows to measure the relative importance of the features. It is calculated by looking at how much the tree nodes that use that feature reduce the impurity on average across all the trees in the forest. 

In our model `transactionAmount` ,`merchantName`, `availableMoney` are top three important features while `transactionyear` , `expirationDateKeyInMatch`, `transactionType` are least important ones


```python
#number of trees
n_trees = [int(x) for x in np.linspace(start = 200, stop = 225, num = 10)]

# max depth
max_depth = [int(x) for x in np.linspace(150, 200, num = 20)]
max_depth.append(None)
# create random grid
random_grid = {
 'n_estimators': n_trees,
 'max_depth': max_depth
 }
# Random search of parameters
RFC_random = RandomizedSearchCV(estimator = RFC, param_distributions = random_grid, n_iter = 10, cv = 2, verbose=2, random_state=42, n_jobs = -1)
# Fit the model
RFC_random.fit(X_train, y_train)
# print results
print(RFC_random.best_params_)
```

    Fitting 2 folds for each of 10 candidates, totalling 20 fits


    [Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.
    [Parallel(n_jobs=-1)]: Done  20 out of  20 | elapsed: 18.7min finished


    {'n_estimators': 213, 'max_depth': 189}


`GridSearch`, is a technique that is used to find the best combination of hyper parameters that fits the dataset well. So here we used ___ combination of values including parameters governing depth of the tree, number of trees and number of features.  

The model gave us following result and using that the final model is fitted. 


```python
print(RFC_random.best_params_)
RFC_N = RandomForestClassifier(n_estimators=RFC_random.best_params_['n_estimators'],max_depth=RFC_random.best_params_['max_depth'])
RFC_N.fit(X_train,y_train)
```

    {'n_estimators': 213, 'max_depth': 189}





    RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',
                           max_depth=189, max_features='auto', max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_impurity_split=None,
                           min_samples_leaf=1, min_samples_split=2,
                           min_weight_fraction_leaf=0.0, n_estimators=213,
                           n_jobs=None, oob_score=False, random_state=None,
                           verbose=0, warm_start=False)




```python
RFC_Npredict = RFC_N.predict(X_test)
RFC_Npredict_prob=RFC_N.predict_proba(X_test)[:,1]

CM_N=confusion_matrix(y_test, RFC_Npredict)   
plot_ConMatrix_ROCcurve(cm=CM_N,RFC_predict_prob=RFC_Npredict_prob)

```


![png](/files/images/output_64_0.png)



```python
roc_auc_score(y_test, RFC_Npredict_prob)
```




    0.8171631917739818



We can see from the output that there was a slight improvement in the results. Specially, the non-classification error. The model is not predicting the autual non fraud transaction as fraudulent ones. That's huge! Further, Our roc_auc score is improved from .68 to .82. We can also notice that the curve far well behaving than the previous one.


```python
#Important features
import_features1=pd.DataFrame({'Feature': list(X_train.columns),
                   'Importance': RFC_N.feature_importances_}).\
                    sort_values('Importance', ascending = False)

# Display
plt.figure(figsize=(20,8))
bplot=sns.barplot(x='Feature',y='Importance',data=import_features1)
_=plt.setp(bplot.get_xticklabels(), rotation=45,horizontalalignment='right')
bplot.set_ylabel('Importance',fontsize=15)
bplot.set_xlabel('Features',fontsize=15)
bplot.set_title("Feature Importance plot",fontsize=20)
```




    Text(0.5, 1.0, 'Feature Importance plot')




![png](/files/images/output_67_1.png)


The feature importance hasn't changed. So, `transactionAmount` ,`merchantName`, `availableMoney` are top three important features while `transactionyear` , `expirationDateKeyInMatch`, `transactionType` are least important ones

## 5. Conclusion

The analysis started by giving the summary statistics, through that some variables with no information are removed. Then the **NULL values in rest of the variables are removed**. Keeping this cleaned dataset, then proceeded into the next step, where we explicitly discussed the **behavior of transaction amount** by means of histogram. We confirmed that this is distributed in a **rightly skewed** fashion, with **huge amount of small transactions**. Moving to next step we have removed further discrepancies in the dataset, which is presented in the form reversed transactions and multi-swap transactions. Through successful implementation of the program we could be able to resolve these issues. So, the **total number of reversed transactions are $20062$** and the **total dollar amount reversed is $\$2787895.03$**. We have used several times intervals to identify the multi-swap transaction, and highly recommended to consider a **time less than 2 minutes**. Next feature engineering is carried out and there we have treated the date-time variables and identified some highly correlated variables. Then, we chose to build the **Random Forest Classification model** it is trained under **cross validations** and various combination of hyper parameters. Thorough this it is identified that **`transactionAmount` ,`merchantName`, `availableMoney`** as top **three important features** while **`transactionyear`, `expirationDateKeyInMatch`, `transactionType`** as **least important ones**. Finally, the model is well tuned using various model evaluation metrics and the accuracy is noticed as **AUC 0.82**. 

We have tried several classification algorithms such as **logistic regression, naïve Bayes** and they are time consuming and therefore we have finally decided to go for Random Forest Classification. Further the **feature importance metric** of Random forest had extremely helpful in answering one the project question, which is *what features/data you found useful?* If we had enough time we could have evaluated the performance this model over **a wide range of hyper-parameter space** of random forest and by means of all the classification models on this dataset. Most importantly, we could have used some advanced and highly efficienct algorithms like **AdaBoost, Gradient Boosting, and XGBoost**


```python

```
